# MEDICAL-CHATBOT-USING-RAG  
ðŸ”¹ Project Flow
Phase 1 â€“ Setup Memory for LLM (Knowledge Base)  
Load medical PDFs (guidelines, research papers, FAQs, etc.)  
Chunk documents into smaller pieces for better retrieval.  
Generate vector embeddings using a transformer-based embedding model.  
Store embeddings in FAISS (Facebook AI Similarity Search) vector database.  

Phase 2 â€“ Connect Memory with LLM  
Set up LLM (Mistral via HuggingFace).  
Integrate LLM with FAISS for retrieval-based QA.  
Create a RAG Chain:  
User query â†’ FAISS (retrieves relevant chunks) â†’ Mistral generates response using context.  

Phase 3 â€“ User Interface
Build Chatbot UI with Streamlit.  
Load FAISS vector store in cache for fast access.  
Enable RAG-powered conversation (retrieval + generation).  

ðŸ”¹ Tools & Tech Stack  
Langchain â†’ Orchestration of RAG pipeline.  
HuggingFace â†’ Models (Mistral + Embeddings).  
FAISS â†’ Vector DB.  
Streamlit â†’ Chat UI.  
Python + VS Code â†’ Development.  

ðŸ”¹Technical Architecture (Simplified Flow)  
User asks a medical question.  
Query goes to RAG pipeline.  
FAISS retrieves top-k relevant medical chunks.  
Mistral LLM uses chunks as context â†’ generates response.  
Answer displayed in Streamlit Chat UI.  
